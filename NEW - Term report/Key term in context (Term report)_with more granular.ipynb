{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58604bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce533da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### parse\n",
    "\n",
    "def parse_xml_to_df(xml_file):\n",
    "    \n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        tree = etree.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Get the filename without the extension\n",
    "        filename = os.path.basename(xml_file)\n",
    "\n",
    "        # Create a list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Iterate over all elements in the XML file\n",
    "        for element in root:\n",
    "            # Create a dictionary to store the data for each element\n",
    "            element_data = {}\n",
    "            \n",
    "            # add the filename\n",
    "            element_data['source_filename'] = filename\n",
    "            \n",
    "            ## extract id\n",
    "            eadid = root.find('.//eadid')\n",
    "            if eadid is not None:\n",
    "                element_data['ead_id'] = eadid.text.strip()  # Add strip() to remove leading and trailing white space\n",
    "            \n",
    "            publicid = eadid.get('publicid')\n",
    "            if publicid is not None:\n",
    "                result = re.search(r'::(.*)\\.xml', publicid)\n",
    "                if result:\n",
    "                    public_id = result.group(1).split('::')[-1]\n",
    "                    element_data['public_id'] = public_id    \n",
    "            \n",
    "            # Extract titleproper\n",
    "            titleproper = root.find('.//titleproper')\n",
    "            if titleproper is not None:\n",
    "                element_data['titleproper'] = titleproper.text\n",
    "            \n",
    "            \n",
    "            ## EXtract abstract\n",
    "            abstract = element.find('.//abstract')\n",
    "            if abstract is not None:\n",
    "                element_data['abstract'] = abstract.text\n",
    "\n",
    "            ## Extract language\n",
    "            language = element.find('.//langmaterial')\n",
    "            if language is not None:\n",
    "                element_data['language'] = ''.join(language.itertext())\n",
    "\n",
    "            ## Extract scopecontent\n",
    "            scopecontent = element.findall('./scopecontent')\n",
    "            if scopecontent:\n",
    "                scopecontent_texts = []\n",
    "                for sc in scopecontent:\n",
    "                    paragraphs = sc.findall('./p')\n",
    "                    if paragraphs:\n",
    "                        for p in paragraphs:\n",
    "                            p_text = \"\"\n",
    "                            for child in p.itertext():\n",
    "                                p_text += child\n",
    "                            scopecontent_texts.append(p_text)\n",
    "                element_data['scopecontent'] = '; '.join(scopecontent_texts)\n",
    "            \n",
    "            ## Extract controlaccess\n",
    "            controlaccess = element.find('.//controlaccess')\n",
    "            if controlaccess is not None:\n",
    "                subjects = controlaccess.findall('.//subject')\n",
    "                if subjects:\n",
    "                    element_data['subjects'] = '; '.join([subject.text for subject in subjects])\n",
    "                    # Extract the 'source' attribute for each 'subject' tag\n",
    "                    element_data['subjects_source'] = '; '.join([subject.get('source') for subject in subjects if subject.get('source') is not None])\n",
    "\n",
    "                genreforms = controlaccess.findall('.//genreform')\n",
    "                if genreforms:\n",
    "                    element_data['genreforms'] = '; '.join([genreform.text for genreform in genreforms])\n",
    "                    # Extract the 'source' attribute for each 'genreform' tag\n",
    "                    element_data['genreforms_source'] = '; '.join([genreform.get('source') for genreform in genreforms if genreform.get('source') is not None])\n",
    "\n",
    "                geognames = controlaccess.findall('.//geogname')\n",
    "                if geognames:\n",
    "                    element_data['geognames'] = '; '.join([geogname.text for geogname in geognames])\n",
    "                    # Extract the 'source' attribute for each 'geogname' tag\n",
    "                    element_data['geognames_source'] = '; '.join([geogname.get('source') for geogname in geognames if geogname.get('source') is not None])\n",
    "\n",
    "                persnames = controlaccess.findall('.//persname')\n",
    "                if persnames:\n",
    "                    element_data['persnames'] = '; '.join([persname.text for persname in persnames])\n",
    "                    # Extract the 'source' attribute for each 'persname' tag\n",
    "                    element_data['persnames_source'] = '; '.join([persname.get('source') for persname in persnames if persname.get('source') is not None])\n",
    "\n",
    "                corpnames = controlaccess.findall('.//corpname')\n",
    "                if corpnames:\n",
    "                    element_data['corpnames'] = '; '.join([corpname.text for corpname in corpnames])\n",
    "                    # Extract the 'source' attribute for each 'corpname' tag\n",
    "                    element_data['corpnames_source'] = '; '.join([corpname.get('source') for corpname in corpnames if corpname.get('source') is not None])\n",
    "\n",
    "                famnames = controlaccess.findall('.//famname')\n",
    "                if famnames:\n",
    "                    element_data['famnames'] = '; '.join([famname.text for famname in famnames])\n",
    "                    # Extract the 'source' attribute for each 'famname' tag\n",
    "                    element_data['famnames_source'] = '; '.join([famname.get('source') for famname in famnames if famname.get('source') is not None])\n",
    "\n",
    "            ## Extract bioghist    \n",
    "            bioghist = element.findall('./bioghist')\n",
    "            if bioghist:\n",
    "                bioghist_texts = []\n",
    "                for bio in bioghist:\n",
    "                    paragraphs = bio.findall('./p')\n",
    "                    if paragraphs:\n",
    "                        for p in paragraphs:\n",
    "                            p_text = \"\"\n",
    "                            for child in p.itertext():\n",
    "                                p_text += child\n",
    "                            bioghist_texts.append(p_text)\n",
    "                element_data['bioghist'] = '; '.join(bioghist_texts)\n",
    "\n",
    "            ## Extract custodhist\n",
    "            custodhist = element.findall('./custodhist')\n",
    "            if custodhist:\n",
    "                custodhist_texts = []\n",
    "                for cus in custodhist:\n",
    "                    paragraphs = cus.findall('./p')\n",
    "                    if paragraphs:\n",
    "                        for p in paragraphs:\n",
    "                            p_text = \"\"\n",
    "                            for child in p.itertext():\n",
    "                                p_text += child\n",
    "                            custodhist_texts.append(p_text)\n",
    "                element_data['custodhist'] = '; '.join(custodhist_texts)\n",
    "\n",
    "            # Add the element data to the list of data\n",
    "            data.append(element_data)\n",
    "\n",
    "        # print(data)\n",
    "        \n",
    "        df = pd.DataFrame([d for d in data if len(d)>4])\n",
    "\n",
    "    except:\n",
    "        # If error, print the error message and skip the file\n",
    "        print(\"Error parsing file:\", xml_file)\n",
    "        df = None\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_xml_folder_to_df(folder_path):\n",
    "    # Create a list to store the dataframes for each file\n",
    "    dfs = []\n",
    "    \n",
    "    # Loop over all XML files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = parse_xml_to_df(file_path)\n",
    "            dfs.append(df)\n",
    "    \n",
    "    # Concatenate the dataframes into one dataframe\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# NEED to customize to your ownï¼šchange the path here to your path\n",
    "df1 = parse_xml_folder_to_df(\"RCRC_Finding_Aid_List_Bentley/Finding_Aids\")\n",
    "df1.to_csv('parse_df1.csv', index=True)\n",
    "\n",
    "\n",
    "\n",
    "## term matching\n",
    "\n",
    "# read in the txt file term list\n",
    "with open('terms_all.txt', 'r') as f:\n",
    "    terms = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "def match_terms(row, terms, columns):\n",
    "    results = []\n",
    "    for term in terms:\n",
    "        for col in columns:\n",
    "            if not isinstance(row[col], float):\n",
    "                # Split the column into paragraphs\n",
    "                paragraphs = row[col].split('\\n')\n",
    "                # Loop through each paragraph\n",
    "                for paragraph in paragraphs:\n",
    "                    # Check if the term is in the current paragraph\n",
    "                    if re.search(r'\\b' + re.escape(term) + r'\\b', paragraph, re.IGNORECASE):\n",
    "                        # Split paragraph into sentences\n",
    "                        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', paragraph)\n",
    "                        # Find the sentence containing the term\n",
    "                        matched_sentence = next((sentence for sentence in sentences if re.search(r'\\b' + re.escape(term) + r'\\b', sentence, re.IGNORECASE)), paragraph)\n",
    "                        results.append({\n",
    "                            'Term': term,\n",
    "                            'Occurrence (ead_ID)': row['ead_id'],\n",
    "                            'Field': col, \n",
    "                            'Collection': row.get('titleproper', None),\n",
    "                            'Context': matched_sentence  # Returning only the matched sentence\n",
    "                        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def match_and_visualize(df, name):\n",
    "    # Match results\n",
    "    results_df = pd.DataFrame([result for index, row in df.iterrows() for result in match_terms(row, terms, df.columns)])\n",
    "    \n",
    "    # Sort results by 'Term'\n",
    "    sorted_results_df = results_df.sort_values(by='Term', ascending=True)\n",
    "    \n",
    "    # Show matched results\n",
    "    print(\"Matched results for \", name)\n",
    "\n",
    "    # Export to CSV without the index\n",
    "    sorted_results_df.to_csv('matched_results_' + name + '.csv', index=False)\n",
    "    return sorted_results_df \n",
    "  \n",
    "\n",
    "# NEED to customize to your ownï¼šget the match results now, change to your data name (e.g., in this example case, we use \"Bentley\")\n",
    "match_and_visualize(df1, 'Bentley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86845d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================== if more granular for example five words preceding and five following the term/ phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9c6dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched results for  Bentley\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Occurrence (ead_ID)</th>\n",
       "      <th>Field</th>\n",
       "      <th>Collection</th>\n",
       "      <th>Context</th>\n",
       "      <th>Context (extended)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Benevolent Assimilation</td>\n",
       "      <td>umich-bhl-86354</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding Aid for Dean C. Worcester Papers</td>\n",
       "      <td>McKinley asked Worcester to join a \"civilian c...</td>\n",
       "      <td>of the U.S. program of Benevolent Assimilation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Colonial</td>\n",
       "      <td>umich-bhl-8772</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding aid for Luce Philippine Project interv...</td>\n",
       "      <td>In 1977 the University of Michigan Center for ...</td>\n",
       "      <td>(former diplomats, teachers, missionaries, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Colonial</td>\n",
       "      <td>umich-bhl-85419</td>\n",
       "      <td>scopecontent</td>\n",
       "      <td>Finding aid for Owen A. Tomlinson papers, 1899...</td>\n",
       "      <td>Within the Photograph series will be found six...</td>\n",
       "      <td>U.S. Army personnel, and other Colonial offici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Colonial</td>\n",
       "      <td>umich-bhl-851733</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding Aid for Harry Burns Hutchins papers</td>\n",
       "      <td>Mary Hutchins was a member of many organizatio...</td>\n",
       "      <td>flag, the Michigan Chapter of Colonial Dames, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Colonial</td>\n",
       "      <td>umich-bhl-851764</td>\n",
       "      <td>abstract</td>\n",
       "      <td>Finding aid for George A. Malcolm papers, 1896...</td>\n",
       "      <td>Correspondence, scrapbooks, printed reports, a...</td>\n",
       "      <td>Colonialism: memoirs of an American Colonial C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Types</td>\n",
       "      <td>umich-bhl-87265.0</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding aid for News and Information Services ...</td>\n",
       "      <td>News Service has continued to expand its media...</td>\n",
       "      <td>continued to expand its media Types and now in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Types</td>\n",
       "      <td>umich-bhl-2014136</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding aid for University Herbarium (Universi...</td>\n",
       "      <td>The U-M Herbarium is also a leader in digitizi...</td>\n",
       "      <td>1977 database of seed plant Types (Reznicek).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Types</td>\n",
       "      <td>umich-bhl-851285</td>\n",
       "      <td>scopecontent</td>\n",
       "      <td>Finding Aid for Thomas Francis Papers</td>\n",
       "      <td>Types of records in these unprocessed subserie...</td>\n",
       "      <td>Types of records in these unprocessed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Types</td>\n",
       "      <td>umich-bhl-85193</td>\n",
       "      <td>scopecontent</td>\n",
       "      <td>Finding Aid for Philip A. Hart Papers</td>\n",
       "      <td>Hart himself and his staff had discarded certa...</td>\n",
       "      <td>his staff had discarded certain Types of files...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Types</td>\n",
       "      <td>umich-bhl-9840</td>\n",
       "      <td>scopecontent</td>\n",
       "      <td>Finding aid for Charles W. Lane papers, 1935-1997</td>\n",
       "      <td>The researcher will be interested in the varie...</td>\n",
       "      <td>mobile homes, churches, and other Types of str...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Term Occurrence (ead_ID)         Field  \\\n",
       "138  Benevolent Assimilation     umich-bhl-86354      bioghist   \n",
       "162                 Colonial      umich-bhl-8772      bioghist   \n",
       "99                  Colonial     umich-bhl-85419  scopecontent   \n",
       "69                  Colonial    umich-bhl-851733      bioghist   \n",
       "72                  Colonial    umich-bhl-851764      abstract   \n",
       "..                       ...                 ...           ...   \n",
       "159                    Types   umich-bhl-87265.0      bioghist   \n",
       "46                     Types   umich-bhl-2014136      bioghist   \n",
       "52                     Types    umich-bhl-851285  scopecontent   \n",
       "82                     Types     umich-bhl-85193  scopecontent   \n",
       "199                    Types      umich-bhl-9840  scopecontent   \n",
       "\n",
       "                                            Collection  \\\n",
       "138           Finding Aid for Dean C. Worcester Papers   \n",
       "162  Finding aid for Luce Philippine Project interv...   \n",
       "99   Finding aid for Owen A. Tomlinson papers, 1899...   \n",
       "69         Finding Aid for Harry Burns Hutchins papers   \n",
       "72   Finding aid for George A. Malcolm papers, 1896...   \n",
       "..                                                 ...   \n",
       "159  Finding aid for News and Information Services ...   \n",
       "46   Finding aid for University Herbarium (Universi...   \n",
       "52               Finding Aid for Thomas Francis Papers   \n",
       "82               Finding Aid for Philip A. Hart Papers   \n",
       "199  Finding aid for Charles W. Lane papers, 1935-1997   \n",
       "\n",
       "                                               Context  \\\n",
       "138  McKinley asked Worcester to join a \"civilian c...   \n",
       "162  In 1977 the University of Michigan Center for ...   \n",
       "99   Within the Photograph series will be found six...   \n",
       "69   Mary Hutchins was a member of many organizatio...   \n",
       "72   Correspondence, scrapbooks, printed reports, a...   \n",
       "..                                                 ...   \n",
       "159  News Service has continued to expand its media...   \n",
       "46   The U-M Herbarium is also a leader in digitizi...   \n",
       "52   Types of records in these unprocessed subserie...   \n",
       "82   Hart himself and his staff had discarded certa...   \n",
       "199  The researcher will be interested in the varie...   \n",
       "\n",
       "                                    Context (extended)  \n",
       "138  of the U.S. program of Benevolent Assimilation...  \n",
       "162  (former diplomats, teachers, missionaries, ser...  \n",
       "99   U.S. Army personnel, and other Colonial offici...  \n",
       "69   flag, the Michigan Chapter of Colonial Dames, ...  \n",
       "72   Colonialism: memoirs of an American Colonial C...  \n",
       "..                                                 ...  \n",
       "159  continued to expand its media Types and now in...  \n",
       "46       1977 database of seed plant Types (Reznicek).  \n",
       "52               Types of records in these unprocessed  \n",
       "82   his staff had discarded certain Types of files...  \n",
       "199  mobile homes, churches, and other Types of str...  \n",
       "\n",
       "[202 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "def parse_xml_to_df(xml_file):\n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        tree = etree.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Get the filename without the extension\n",
    "        filename = os.path.basename(xml_file)\n",
    "\n",
    "        # Create a list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Iterate over all elements in the XML file\n",
    "        for element in root:\n",
    "            # Create a dictionary to store the data for each element\n",
    "            element_data = {}\n",
    "            \n",
    "            # add the filename\n",
    "            element_data['source_filename'] = filename\n",
    "            \n",
    "            ## extract id\n",
    "            eadid = root.find('.//eadid')\n",
    "            if eadid is not None:\n",
    "                element_data['ead_id'] = eadid.text.strip()  # Add strip() to remove leading and trailing white space\n",
    "            \n",
    "            publicid = eadid.get('publicid')\n",
    "            if publicid is not None:\n",
    "                result = re.search(r'::(.*)\\.xml', publicid)\n",
    "                if result:\n",
    "                    public_id = result.group(1).split('::')[-1]\n",
    "                    element_data['public_id'] = public_id    \n",
    "            \n",
    "            # Extract titleproper\n",
    "            titleproper = root.find('.//titleproper')\n",
    "            if titleproper is not None:\n",
    "                element_data['titleproper'] = titleproper.text\n",
    "            \n",
    "            \n",
    "            ## Extract abstract\n",
    "            abstract = element.find('.//abstract')\n",
    "            if abstract is not None:\n",
    "                element_data['abstract'] = abstract.text\n",
    "\n",
    "            ## Extract language\n",
    "            language = element.find('.//langmaterial')\n",
    "            if language is not None:\n",
    "                element_data['language'] = ''.join(language.itertext())\n",
    "\n",
    "            ## Extract scopecontent\n",
    "            scopecontent = element.findall('./scopecontent')\n",
    "            if scopecontent:\n",
    "                scopecontent_texts = []\n",
    "                for sc in scopecontent:\n",
    "                    paragraphs = sc.findall('./p')\n",
    "                    if paragraphs:\n",
    "                        for p in paragraphs:\n",
    "                            p_text = \"\"\n",
    "                            for child in p.itertext():\n",
    "                                p_text += child\n",
    "                            scopecontent_texts.append(p_text)\n",
    "                element_data['scopecontent'] = '; '.join(scopecontent_texts)\n",
    "            \n",
    "            ## Extract controlaccess\n",
    "            controlaccess = element.find('.//controlaccess')\n",
    "            if controlaccess is not None:\n",
    "                subjects = controlaccess.findall('.//subject')\n",
    "                if subjects:\n",
    "                    element_data['subjects'] = '; '.join([subject.text for subject in subjects])\n",
    "                    # Extract the 'source' attribute for each 'subject' tag\n",
    "                    element_data['subjects_source'] = '; '.join([subject.get('source') for subject in subjects if subject.get('source') is not None])\n",
    "\n",
    "                genreforms = controlaccess.findall('.//genreform')\n",
    "                if genreforms:\n",
    "                    element_data['genreforms'] = '; '.join([genreform.text for genreform in genreforms])\n",
    "                    # Extract the 'source' attribute for each 'genreform' tag\n",
    "                    element_data['genreforms_source'] = '; '.join([genreform.get('source') for genreform in genreforms if genreform.get('source') is not None])\n",
    "\n",
    "                geognames = controlaccess.findall('.//geogname')\n",
    "                if geognames:\n",
    "                    element_data['geognames'] = '; '.join([geogname.text for geogname in geognames])\n",
    "                    # Extract the 'source' attribute for each 'geogname' tag\n",
    "                    element_data['geognames_source'] = '; '.join([geogname.get('source') for geogname in geognames if geogname.get('source') is not None])\n",
    "\n",
    "                persnames = controlaccess.findall('.//persname')\n",
    "                if persnames:\n",
    "                    element_data['persnames'] = '; '.join([persname.text for persname in persnames])\n",
    "                    # Extract the 'source' attribute for each 'persname' tag\n",
    "                    element_data['persnames_source'] = '; '.join([persname.get('source') for persname in persnames if persname.get('source') is not None])\n",
    "\n",
    "                corpnames = controlaccess.findall('.//corpname')\n",
    "                if corpnames:\n",
    "                    element_data['corpnames'] = '; '.join([corpname.text for corpname in corpnames])\n",
    "                    # Extract the 'source' attribute for each 'corpname' tag\n",
    "                    element_data['corpnames_source'] = '; '.join([corpname.get('source') for corpname in corpnames if corpname.get('source') is not None])\n",
    "\n",
    "                famnames = controlaccess.findall('.//famname')\n",
    "                if famnames:\n",
    "                    element_data['famnames'] = '; '.join([famname.text for famname in famnames])\n",
    "                    # Extract the 'source' attribute for each 'famname' tag\n",
    "                    element_data['famnames_source'] = '; '.join([famname.get('source') for famname in famnames if famname.get('source') is not None])\n",
    "\n",
    "            ## Extract bioghist    \n",
    "            bioghist = element.findall('./bioghist')\n",
    "            if bioghist:\n",
    "                bioghist_texts = []\n",
    "                for bio in bioghist:\n",
    "                    paragraphs = bio.findall('./p')\n",
    "                    if paragraphs:\n",
    "                        for p in paragraphs:\n",
    "                            p_text = \"\"\n",
    "                            for child in p.itertext():\n",
    "                                p_text += child\n",
    "                            bioghist_texts.append(p_text)\n",
    "                element_data['bioghist'] = '; '.join(bioghist_texts)\n",
    "\n",
    "            ## Extract custodhist\n",
    "            custodhist = element.findall('./custodhist')\n",
    "            if custodhist:\n",
    "                custodhist_texts = []\n",
    "                for cus in custodhist:\n",
    "                    paragraphs = cus.findall('./p')\n",
    "                    if paragraphs:\n",
    "                        for p in paragraphs:\n",
    "                            p_text = \"\"\n",
    "                            for child in p.itertext():\n",
    "                                p_text += child\n",
    "                            custodhist_texts.append(p_text)\n",
    "                element_data['custodhist'] = '; '.join(custodhist_texts)\n",
    "\n",
    "            # Add the element data to the list of data\n",
    "            data.append(element_data)\n",
    "\n",
    "        # print(data)\n",
    "        \n",
    "        df = pd.DataFrame([d for d in data if len(d)>4])\n",
    "\n",
    "    except Exception as e:\n",
    "        # If error, print the error message and skip the file\n",
    "        print(\"Error parsing file:\", xml_file, e)\n",
    "        df = None\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_xml_folder_to_df(folder_path):\n",
    "    # Create a list to store the dataframes for each file\n",
    "    dfs = []\n",
    "    \n",
    "    # Loop over all XML files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = parse_xml_to_df(file_path)\n",
    "            if df is not None:\n",
    "                dfs.append(df)\n",
    "    \n",
    "    # Concatenate the dataframes into one dataframe\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# NEED to customize to your ownï¼šchange the path here to your path\n",
    "df1 = parse_xml_folder_to_df(\"RCRC_Finding_Aid_List_Bentley/Finding_Aids\")\n",
    "df1.to_csv('parse_df1.csv', index=True)\n",
    "\n",
    "\n",
    "\n",
    "## term matching\n",
    "\n",
    "# read in the txt file term list\n",
    "with open('terms_all.txt', 'r') as f:\n",
    "    terms = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "def match_terms(row, terms, columns):\n",
    "    results = []\n",
    "    for term in terms:\n",
    "        for col in columns:\n",
    "            if not isinstance(row[col], float):\n",
    "                # Split the column into paragraphs\n",
    "                paragraphs = row[col].split('\\n')\n",
    "                # Loop through each paragraph\n",
    "                for paragraph in paragraphs:\n",
    "                    # Check if the term is in the current paragraph\n",
    "                    if re.search(r'\\b' + re.escape(term) + r'\\b', paragraph, re.IGNORECASE):\n",
    "                        # Split paragraph into sentences\n",
    "                        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', paragraph)\n",
    "                        # Find the sentence containing the term\n",
    "                        matched_sentence = next((sentence for sentence in sentences if re.search(r'\\b' + re.escape(term) + r'\\b', sentence, re.IGNORECASE)), paragraph)\n",
    "\n",
    "                        # Extract context: five words before and after the term\n",
    "                        words = matched_sentence.split()\n",
    "                        # Handle multi-word terms by finding the entire phrase in the sentence\n",
    "                        pattern = re.compile(r'\\b' + re.escape(term) + r'\\b', re.IGNORECASE)\n",
    "                        matches = list(pattern.finditer(matched_sentence))\n",
    "                        for match in matches:\n",
    "                            start_index = match.start()\n",
    "                            end_index = match.end()\n",
    "                            # Split the sentence around the matched phrase\n",
    "                            pre_context = matched_sentence[:start_index].strip().split()[-5:]  # last five words before the match\n",
    "                            post_context = matched_sentence[end_index:].strip().split()[:5]    # first five words after the match\n",
    "                            context = ' '.join(pre_context + [term] + post_context)\n",
    "\n",
    "                            results.append({\n",
    "                                'Term': term,\n",
    "                                'Occurrence (ead_ID)': row['ead_id'],\n",
    "                                'Field': col, \n",
    "                                'Collection': row.get('titleproper', None),\n",
    "                                'Context': matched_sentence,  # Original matched sentence\n",
    "                                'Context (extended)': context  # New context with five words before and after\n",
    "                            })\n",
    "    return results\n",
    "\n",
    "\n",
    "def match_and_visualize(df, name):\n",
    "    # Match results\n",
    "    results_df = pd.DataFrame([result for index, row in df.iterrows() for result in match_terms(row, terms, df.columns)])\n",
    "    \n",
    "    # Sort results by 'Term'\n",
    "    sorted_results_df = results_df.sort_values(by='Term', ascending=True)\n",
    "    \n",
    "    # Show matched results\n",
    "    print(\"Matched results for \", name)\n",
    "\n",
    "    # Export to CSV without the index\n",
    "    sorted_results_df.to_csv('matched_results_' + name + '.csv', index=False)\n",
    "    return sorted_results_df \n",
    "  \n",
    "\n",
    "# NEED to customize to your ownï¼šget the match results now, change to your data name (e.g., in this example case, we use \"Bentley\")\n",
    "matched_df = match_and_visualize(df1, 'Bentley')\n",
    "\n",
    "# Display the result\n",
    "matched_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7c4da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
