{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58604bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce533da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched results for  Bentley\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Occurrence (ead_ID)</th>\n",
       "      <th>Field</th>\n",
       "      <th>Collection</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Benevolent Assimilation</td>\n",
       "      <td>umich-bhl-86354</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding Aid for Dean C. Worcester Papers</td>\n",
       "      <td>McKinley asked Worcester to join a \"civilian c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Colonial</td>\n",
       "      <td>umich-bhl-86354</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding Aid for Dean C. Worcester Papers</td>\n",
       "      <td>Worcester's influence on American colonial pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Colonial</td>\n",
       "      <td>umich-bhl-8868</td>\n",
       "      <td>scopecontent</td>\n",
       "      <td>Finding aid for Blanchard Family Papers, ca. 1...</td>\n",
       "      <td>The Blanchard Family Papers will be of value t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Colonial</td>\n",
       "      <td>umich-bhl-8772</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding aid for Luce Philippine Project interv...</td>\n",
       "      <td>In 1977 the University of Michigan Center for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Colonial</td>\n",
       "      <td>umich-bhl-851733</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding Aid for Harry Burns Hutchins papers</td>\n",
       "      <td>Mary Hutchins was a member of many organizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Types</td>\n",
       "      <td>umich-bhl-851285</td>\n",
       "      <td>scopecontent</td>\n",
       "      <td>Finding Aid for Thomas Francis Papers</td>\n",
       "      <td>Types of records in these unprocessed subserie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Types</td>\n",
       "      <td>umich-bhl-85193</td>\n",
       "      <td>scopecontent</td>\n",
       "      <td>Finding Aid for Philip A. Hart Papers</td>\n",
       "      <td>Hart himself and his staff had discarded certa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Types</td>\n",
       "      <td>umich-bhl-87265.25</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding aid for News and Information Services ...</td>\n",
       "      <td>News Service has continued to expand its media...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Types</td>\n",
       "      <td>umich-bhl-9840</td>\n",
       "      <td>scopecontent</td>\n",
       "      <td>Finding aid for Charles W. Lane papers, 1935-1997</td>\n",
       "      <td>The researcher will be interested in the varie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Types</td>\n",
       "      <td>umich-bhl-87265.0</td>\n",
       "      <td>bioghist</td>\n",
       "      <td>Finding aid for News and Information Services ...</td>\n",
       "      <td>News Service has continued to expand its media...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Term Occurrence (ead_ID)         Field  \\\n",
       "123  Benevolent Assimilation     umich-bhl-86354      bioghist   \n",
       "124                 Colonial     umich-bhl-86354      bioghist   \n",
       "152                 Colonial      umich-bhl-8868  scopecontent   \n",
       "147                 Colonial      umich-bhl-8772      bioghist   \n",
       "57                  Colonial    umich-bhl-851733      bioghist   \n",
       "..                       ...                 ...           ...   \n",
       "42                     Types    umich-bhl-851285  scopecontent   \n",
       "69                     Types     umich-bhl-85193  scopecontent   \n",
       "145                    Types  umich-bhl-87265.25      bioghist   \n",
       "183                    Types      umich-bhl-9840  scopecontent   \n",
       "144                    Types   umich-bhl-87265.0      bioghist   \n",
       "\n",
       "                                            Collection  \\\n",
       "123           Finding Aid for Dean C. Worcester Papers   \n",
       "124           Finding Aid for Dean C. Worcester Papers   \n",
       "152  Finding aid for Blanchard Family Papers, ca. 1...   \n",
       "147  Finding aid for Luce Philippine Project interv...   \n",
       "57         Finding Aid for Harry Burns Hutchins papers   \n",
       "..                                                 ...   \n",
       "42               Finding Aid for Thomas Francis Papers   \n",
       "69               Finding Aid for Philip A. Hart Papers   \n",
       "145  Finding aid for News and Information Services ...   \n",
       "183  Finding aid for Charles W. Lane papers, 1935-1997   \n",
       "144  Finding aid for News and Information Services ...   \n",
       "\n",
       "                                               Context  \n",
       "123  McKinley asked Worcester to join a \"civilian c...  \n",
       "124  Worcester's influence on American colonial pol...  \n",
       "152  The Blanchard Family Papers will be of value t...  \n",
       "147  In 1977 the University of Michigan Center for ...  \n",
       "57   Mary Hutchins was a member of many organizatio...  \n",
       "..                                                 ...  \n",
       "42   Types of records in these unprocessed subserie...  \n",
       "69   Hart himself and his staff had discarded certa...  \n",
       "145  News Service has continued to expand its media...  \n",
       "183  The researcher will be interested in the varie...  \n",
       "144  News Service has continued to expand its media...  \n",
       "\n",
       "[186 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### parse\n",
    "\n",
    "def parse_xml_to_df(xml_file):\n",
    "    \n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        tree = etree.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Get the filename without the extension\n",
    "        filename = os.path.basename(xml_file)\n",
    "\n",
    "        # Create a list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Iterate over all elements in the XML file\n",
    "        for element in root:\n",
    "            # Create a dictionary to store the data for each element\n",
    "            element_data = {}\n",
    "            \n",
    "            # add the filename\n",
    "            element_data['source_filename'] = filename\n",
    "            \n",
    "            ## extract id\n",
    "            eadid = root.find('.//eadid')\n",
    "            if eadid is not None:\n",
    "                element_data['ead_id'] = eadid.text.strip()  # Add strip() to remove leading and trailing white space\n",
    "            \n",
    "            publicid = eadid.get('publicid')\n",
    "            if publicid is not None:\n",
    "                result = re.search(r'::(.*)\\.xml', publicid)\n",
    "                if result:\n",
    "                    public_id = result.group(1).split('::')[-1]\n",
    "                    element_data['public_id'] = public_id    \n",
    "            \n",
    "            # Extract titleproper\n",
    "            titleproper = root.find('.//titleproper')\n",
    "            if titleproper is not None:\n",
    "                element_data['titleproper'] = titleproper.text\n",
    "            \n",
    "            \n",
    "            ## EXtract abstract\n",
    "            abstract = element.find('.//abstract')\n",
    "            if abstract is not None:\n",
    "                element_data['abstract'] = abstract.text\n",
    "\n",
    "            ## Extract language\n",
    "            language = element.find('.//langmaterial')\n",
    "            if language is not None:\n",
    "                element_data['language'] = ''.join(language.itertext())\n",
    "\n",
    "            ## Extract scopecontent\n",
    "            scopecontent = element.findall('./scopecontent')\n",
    "            if scopecontent:\n",
    "                scopecontent_texts = []\n",
    "                for sc in scopecontent:\n",
    "                    paragraphs = sc.findall('./p')\n",
    "                    if paragraphs:\n",
    "                        for p in paragraphs:\n",
    "                            p_text = \"\"\n",
    "                            for child in p.itertext():\n",
    "                                p_text += child\n",
    "                            scopecontent_texts.append(p_text)\n",
    "                element_data['scopecontent'] = '; '.join(scopecontent_texts)\n",
    "            \n",
    "            ## Extract controlaccess\n",
    "            controlaccess = element.find('.//controlaccess')\n",
    "            if controlaccess is not None:\n",
    "                subjects = controlaccess.findall('.//subject')\n",
    "                if subjects:\n",
    "                    element_data['subjects'] = '; '.join([subject.text for subject in subjects])\n",
    "                    # Extract the 'source' attribute for each 'subject' tag\n",
    "                    element_data['subjects_source'] = '; '.join([subject.get('source') for subject in subjects if subject.get('source') is not None])\n",
    "\n",
    "                genreforms = controlaccess.findall('.//genreform')\n",
    "                if genreforms:\n",
    "                    element_data['genreforms'] = '; '.join([genreform.text for genreform in genreforms])\n",
    "                    # Extract the 'source' attribute for each 'genreform' tag\n",
    "                    element_data['genreforms_source'] = '; '.join([genreform.get('source') for genreform in genreforms if genreform.get('source') is not None])\n",
    "\n",
    "                geognames = controlaccess.findall('.//geogname')\n",
    "                if geognames:\n",
    "                    element_data['geognames'] = '; '.join([geogname.text for geogname in geognames])\n",
    "                    # Extract the 'source' attribute for each 'geogname' tag\n",
    "                    element_data['geognames_source'] = '; '.join([geogname.get('source') for geogname in geognames if geogname.get('source') is not None])\n",
    "\n",
    "                persnames = controlaccess.findall('.//persname')\n",
    "                if persnames:\n",
    "                    element_data['persnames'] = '; '.join([persname.text for persname in persnames])\n",
    "                    # Extract the 'source' attribute for each 'persname' tag\n",
    "                    element_data['persnames_source'] = '; '.join([persname.get('source') for persname in persnames if persname.get('source') is not None])\n",
    "\n",
    "                corpnames = controlaccess.findall('.//corpname')\n",
    "                if corpnames:\n",
    "                    element_data['corpnames'] = '; '.join([corpname.text for corpname in corpnames])\n",
    "                    # Extract the 'source' attribute for each 'corpname' tag\n",
    "                    element_data['corpnames_source'] = '; '.join([corpname.get('source') for corpname in corpnames if corpname.get('source') is not None])\n",
    "\n",
    "                famnames = controlaccess.findall('.//famname')\n",
    "                if famnames:\n",
    "                    element_data['famnames'] = '; '.join([famname.text for famname in famnames])\n",
    "                    # Extract the 'source' attribute for each 'famname' tag\n",
    "                    element_data['famnames_source'] = '; '.join([famname.get('source') for famname in famnames if famname.get('source') is not None])\n",
    "\n",
    "            ## Extract bioghist    \n",
    "            bioghist = element.findall('./bioghist')\n",
    "            if bioghist:\n",
    "                bioghist_texts = []\n",
    "                for bio in bioghist:\n",
    "                    paragraphs = bio.findall('./p')\n",
    "                    if paragraphs:\n",
    "                        for p in paragraphs:\n",
    "                            p_text = \"\"\n",
    "                            for child in p.itertext():\n",
    "                                p_text += child\n",
    "                            bioghist_texts.append(p_text)\n",
    "                element_data['bioghist'] = '; '.join(bioghist_texts)\n",
    "\n",
    "            ## Extract custodhist\n",
    "            custodhist = element.findall('./custodhist')\n",
    "            if custodhist:\n",
    "                custodhist_texts = []\n",
    "                for cus in custodhist:\n",
    "                    paragraphs = cus.findall('./p')\n",
    "                    if paragraphs:\n",
    "                        for p in paragraphs:\n",
    "                            p_text = \"\"\n",
    "                            for child in p.itertext():\n",
    "                                p_text += child\n",
    "                            custodhist_texts.append(p_text)\n",
    "                element_data['custodhist'] = '; '.join(custodhist_texts)\n",
    "\n",
    "            # Add the element data to the list of data\n",
    "            data.append(element_data)\n",
    "\n",
    "        # print(data)\n",
    "        \n",
    "        df = pd.DataFrame([d for d in data if len(d)>4])\n",
    "\n",
    "    except:\n",
    "        # If error, print the error message and skip the file\n",
    "        print(\"Error parsing file:\", xml_file)\n",
    "        df = None\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_xml_folder_to_df(folder_path):\n",
    "    # Create a list to store the dataframes for each file\n",
    "    dfs = []\n",
    "    \n",
    "    # Loop over all XML files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = parse_xml_to_df(file_path)\n",
    "            dfs.append(df)\n",
    "    \n",
    "    # Concatenate the dataframes into one dataframe\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# NEED to customize to your ownï¼šchange the path here to your path\n",
    "df1 = parse_xml_folder_to_df(\"RCRC_Finding_Aid_List_Bentley/Finding_Aids\")\n",
    "df1.to_csv('parse_df1.csv', index=True)\n",
    "\n",
    "\n",
    "\n",
    "## term matching\n",
    "\n",
    "# read in the txt file term list\n",
    "with open('terms_all.txt', 'r') as f:\n",
    "    terms = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "def match_terms(row, terms, columns):\n",
    "    results = []\n",
    "    for term in terms:\n",
    "        for col in columns:\n",
    "            if not isinstance(row[col], float):\n",
    "                # Split the column into paragraphs\n",
    "                paragraphs = row[col].split('\\n')\n",
    "                # Loop through each paragraph\n",
    "                for paragraph in paragraphs:\n",
    "                    # Check if the term is in the current paragraph\n",
    "                    if re.search(r'\\b' + re.escape(term) + r'\\b', paragraph, re.IGNORECASE):\n",
    "                        # Split paragraph into sentences\n",
    "                        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', paragraph)\n",
    "                        # Find the sentence containing the term\n",
    "                        matched_sentence = next((sentence for sentence in sentences if re.search(r'\\b' + re.escape(term) + r'\\b', sentence, re.IGNORECASE)), paragraph)\n",
    "                        results.append({\n",
    "                            'Term': term,\n",
    "                            'Occurrence (ead_ID)': row['ead_id'],\n",
    "                            'Field': col, \n",
    "                            'Collection': row.get('titleproper', None),\n",
    "                            'Context': matched_sentence  # Returning only the matched sentence\n",
    "                        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def match_and_visualize(df, name):\n",
    "    # Match results\n",
    "    results_df = pd.DataFrame([result for index, row in df.iterrows() for result in match_terms(row, terms, df.columns)])\n",
    "    \n",
    "    # Sort results by 'Term'\n",
    "    sorted_results_df = results_df.sort_values(by='Term', ascending=True)\n",
    "    \n",
    "    # Show matched results\n",
    "    print(\"Matched results for \", name)\n",
    "\n",
    "    # Export to CSV without the index\n",
    "    sorted_results_df.to_csv('matched_results_' + name + '.csv', index=False)\n",
    "    return sorted_results_df \n",
    "  \n",
    "\n",
    "# NEED to customize to your ownï¼šget the match results now, change to your data name (e.g., in this example case, we use \"Bentley\")\n",
    "match_and_visualize(df1, 'Bentley')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
